# Quickstart Guide: Obsrv API MVP

**Feature**: Obsrv API - E-commerce Monitoring System MVP
**Branch**: `001-obsrv-api-mvp`
**Date**: 2025-11-03

## Overview

This quickstart guide provides step-by-step instructions for deploying the Obsrv API MVP on a single VPS using Docker Compose. Total setup time: ~30 minutes.

## Prerequisites

### System Requirements

- **VPS Specifications**:
  - 4 CPU cores
  - 8 GB RAM
  - 100 GB storage
  - Ubuntu 22.04 LTS or later (recommended)
  - Public IPv4 address

### Software Requirements

- Docker 24.0+ and Docker Compose 2.20+
- Git 2.30+
- Python 3.11+ (for development/testing)
- OpenSSL (for SSL certificate generation)

### Network Requirements

- Ports to expose:
  - `80` - HTTP (redirects to HTTPS)
  - `443` - HTTPS (API endpoints)
  - `5555` - Celery Flower (monitoring dashboard, optional)
- Firewall rules allowing outbound HTTPS (443) to target e-commerce websites
- Stable internet connection for web crawling

---

## Quick Start (Production)

### 1. Install Docker and Docker Compose

```bash
# Update package index
sudo apt-get update

# Install prerequisites
sudo apt-get install -y \
    ca-certificates \
    curl \
    gnupg \
    lsb-release

# Add Docker GPG key
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

# Add Docker repository
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Install Docker Engine and Docker Compose
sudo apt-get update
sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# Add current user to docker group
sudo usermod -aG docker $USER

# Log out and back in for group changes to take effect
```

Verify installation:
```bash
docker --version  # Should show 24.0+
docker compose version  # Should show 2.20+
```

---

### 2. Clone Repository and Setup Environment

```bash
# Clone repository
git clone https://github.com/your-org/obsrv-api.git
cd obsrv-api

# Checkout feature branch
git checkout 001-obsrv-api-mvp

# Copy environment template
cp .env.example .env

# Generate secure secrets
./scripts/generate-secrets.sh  # Creates random secrets for database, Redis, etc.

# Edit .env with your production values
nano .env
```

---

### 3. Configure Environment Variables

Edit `.env` file with production values:

```bash
# Application
APP_NAME=obsrv-api
APP_ENV=production
DEBUG=false
SECRET_KEY=<auto-generated-32-char-secret>

# API Server
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
API_BASE_URL=https://api.obsrv.example.com

# Database (PostgreSQL)
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=obsrv
POSTGRES_USER=obsrv
POSTGRES_PASSWORD=<auto-generated-secure-password>
DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

# Redis
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=<auto-generated-secure-password>
REDIS_URL=redis://:${REDIS_PASSWORD}@${REDIS_HOST}:${REDIS_PORT}/0

# Celery
CELERY_BROKER_URL=${REDIS_URL}
CELERY_RESULT_BACKEND=${REDIS_URL}
CELERY_WORKER_CONCURRENCY=4
CELERY_BEAT_SCHEDULER=django_celery_beat.schedulers:DatabaseScheduler

# Crawling
CRAWL_USER_AGENT=Mozilla/5.0 (compatible; ObsrvBot/1.0; +https://obsrv.example.com/bot)
CRAWL_RATE_LIMIT_PER_DOMAIN=10  # requests per minute
CRAWL_TIMEOUT_SECONDS=30
CRAWL_MAX_RETRIES=3

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
SENTRY_DSN=  # Optional: Add Sentry DSN for error tracking

# Security
ALLOWED_HOSTS=api.obsrv.example.com,localhost
CORS_ALLOWED_ORIGINS=https://obsrv.example.com
API_KEY_BCRYPT_ROUNDS=12

# Retention
DEFAULT_RETENTION_DAYS=90
MAX_RETENTION_DAYS=365
```

**Security Note**: Ensure all auto-generated passwords are strong (32+ characters, random alphanumeric + symbols).

---

### 4. Setup SSL Certificates (Production)

#### Option A: Let's Encrypt (Recommended)

```bash
# Install Certbot
sudo apt-get install -y certbot

# Generate certificate (HTTP-01 challenge)
sudo certbot certonly --standalone \
  -d api.obsrv.example.com \
  --email admin@obsrv.example.com \
  --agree-tos \
  --non-interactive

# Certificates will be at:
# /etc/letsencrypt/live/api.obsrv.example.com/fullchain.pem
# /etc/letsencrypt/live/api.obsrv.example.com/privkey.pem

# Setup auto-renewal
sudo systemctl enable certbot.timer
sudo systemctl start certbot.timer
```

#### Option B: Self-Signed Certificate (Development/Testing Only)

```bash
mkdir -p ./certs
openssl req -x509 -newkey rsa:4096 -nodes \
  -keyout ./certs/privkey.pem \
  -out ./certs/fullchain.pem \
  -days 365 \
  -subj "/CN=api.obsrv.localhost"
```

Update `docker-compose.prod.yml` to mount certificate paths:
```yaml
services:
  api:
    volumes:
      - /etc/letsencrypt/live/api.obsrv.example.com:/etc/letsencrypt/live/api.obsrv.example.com:ro
```

---

### 5. Initialize Database

```bash
# Start PostgreSQL service only
docker compose -f docker-compose.prod.yml up -d postgres

# Wait for PostgreSQL to be ready
docker compose -f docker-compose.prod.yml exec postgres pg_isready -U obsrv

# Run database migrations
docker compose -f docker-compose.prod.yml run --rm api alembic upgrade head

# Create partitions for product_history table
docker compose -f docker-compose.prod.yml run --rm api python scripts/create-partitions.py --months 12

# (Optional) Seed test data
docker compose -f docker-compose.prod.yml run --rm api python scripts/seed-dev-data.py
```

---

### 6. Start All Services

```bash
# Start all services
docker compose -f docker-compose.prod.yml up -d

# Verify all containers are running
docker compose -f docker-compose.prod.yml ps

# Expected output:
# NAME                  STATUS    PORTS
# obsrv-api-api         Up        0.0.0.0:8000->8000/tcp
# obsrv-api-celery-worker   Up
# obsrv-api-celery-beat     Up
# obsrv-api-postgres        Up        5432/tcp
# obsrv-api-redis           Up        6379/tcp
# obsrv-api-flower          Up        0.0.0.0:5555->5555/tcp

# Check logs
docker compose -f docker-compose.prod.yml logs -f api
```

---

### 7. Verify Deployment

```bash
# Health check
curl https://api.obsrv.example.com/v1/health

# Expected response:
# {
#   "status": "healthy",
#   "timestamp": "2025-11-03T10:00:00Z"
# }

# Detailed health check (requires API key)
curl -H "X-API-Key: <your-api-key>" https://api.obsrv.example.com/v1/health/detailed
```

---

### 8. Create First Client and API Key

```bash
# Access PostgreSQL
docker compose -f docker-compose.prod.yml exec postgres psql -U obsrv -d obsrv

# Create first client
INSERT INTO clients (id, name, email, webhook_secret_current, subscription_tier)
VALUES (
  gen_random_uuid(),
  'Demo Client',
  'demo@example.com',
  encode(gen_random_bytes(48), 'base64'),
  'basic'
);

# Generate API key (use Python script)
docker compose -f docker-compose.prod.yml run --rm api python scripts/generate-api-key.py --client-email demo@example.com

# Output:
# API Key: obsrv_live_a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8
# Client ID: <uuid>
# Copy this key now - it will not be shown again!
```

---

### 9. Register First Website

```bash
# Register website with seed URLs
curl -X POST https://api.obsrv.example.com/v1/websites \
  -H "X-API-Key: obsrv_live_a1b2c3d4..." \
  -H "Content-Type: application/json" \
  -d '{
    "base_url": "https://example-shop.com",
    "seed_urls": [
      "https://example-shop.com/category/electronics"
    ],
    "crawl_frequency_minutes": 1440,
    "price_change_threshold_pct": 1.0,
    "webhook_endpoint_url": "https://your-erp.example.com/webhooks/obsrv"
  }'

# Response:
# {
#   "website_id": "<uuid>",
#   "status": "pending_approval",
#   "message": "Website registered, product discovery in progress",
#   "discovery_job_id": "<uuid>"
# }
```

---

### 10. Monitor Crawl Execution

Access Celery Flower dashboard for real-time task monitoring:

```bash
# Open in browser
https://api.obsrv.example.com:5555

# Login credentials (set in .env)
# Username: admin
# Password: <FLOWER_PASSWORD from .env>
```

**Dashboard Features**:
- Task execution status and history
- Worker health and resource usage
- Retry queue monitoring
- Failed task inspection

---

## Development Setup

### 1. Local Development with Docker Compose

```bash
# Clone repository
git clone https://github.com/your-org/obsrv-api.git
cd obsrv-api
git checkout 001-obsrv-api-mvp

# Copy development environment
cp .env.dev.example .env

# Start development stack
docker compose -f docker-compose.dev.yml up -d

# Run migrations
docker compose -f docker-compose.dev.yml exec api alembic upgrade head

# Access services
# API: http://localhost:8000
# Flower: http://localhost:5555
# PostgreSQL: localhost:5432
# Redis: localhost:6379
```

### 2. Local Development without Docker (Python Virtual Environment)

```bash
# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Install crawl4ai separately (large dependencies)
pip install crawl4ai[all]

# Setup PostgreSQL and Redis locally or use Docker services
docker compose -f docker-compose.dev.yml up -d postgres redis

# Run migrations
export DATABASE_URL="postgresql://obsrv:password@localhost:5432/obsrv"
alembic upgrade head

# Start API server
export REDIS_URL="redis://localhost:6379/0"
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000

# Start Celery worker (separate terminal)
celery -A src.tasks worker --loglevel=info --concurrency=2

# Start Celery Beat scheduler (separate terminal)
celery -A src.tasks beat --loglevel=info
```

### 3. Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html --cov-report=term

# Run specific test categories
pytest tests/unit/              # Unit tests only
pytest tests/integration/       # Integration tests
pytest tests/contract/          # API contract validation

# Run tests in Docker
docker compose -f docker-compose.dev.yml run --rm api pytest
```

---

## Common Operations

### View Logs

```bash
# All services
docker compose -f docker-compose.prod.yml logs -f

# Specific service
docker compose -f docker-compose.prod.yml logs -f api
docker compose -f docker-compose.prod.yml logs -f celery-worker

# Last 100 lines
docker compose -f docker-compose.prod.yml logs --tail=100 api
```

### Restart Services

```bash
# Restart all services
docker compose -f docker-compose.prod.yml restart

# Restart specific service
docker compose -f docker-compose.prod.yml restart api
docker compose -f docker-compose.prod.yml restart celery-worker
```

### Database Backup

```bash
# Backup database
docker compose -f docker-compose.prod.yml exec postgres pg_dump -U obsrv obsrv | gzip > backup-$(date +%Y%m%d-%H%M%S).sql.gz

# Restore database
gunzip < backup-20251103-120000.sql.gz | docker compose -f docker-compose.prod.yml exec -T postgres psql -U obsrv obsrv
```

### Scaling Celery Workers

```bash
# Scale to 4 workers
docker compose -f docker-compose.prod.yml up -d --scale celery-worker=4

# Verify scaling
docker compose -f docker-compose.prod.yml ps celery-worker
```

### Manual Crawl Trigger

```bash
# Trigger immediate crawl for website
curl -X POST https://api.obsrv.example.com/v1/websites/<website-id>/crawl \
  -H "X-API-Key: <your-api-key>"
```

---

## Monitoring and Maintenance

### Health Checks

```bash
# Simple health check
curl https://api.obsrv.example.com/v1/health

# Detailed health (includes database, Redis, Celery)
curl -H "X-API-Key: <your-api-key>" https://api.obsrv.example.com/v1/health/detailed
```

### Resource Monitoring

```bash
# Docker stats
docker stats

# Specific container
docker stats obsrv-api-api

# Disk usage
docker system df
df -h
```

### Log Rotation

Add to `/etc/logrotate.d/docker-containers`:

```bash
/var/lib/docker/containers/*/*.log {
  rotate 7
  daily
  compress
  missingok
  delaycompress
  copytruncate
  maxsize 100M
}
```

### Database Maintenance

```bash
# Run VACUUM ANALYZE weekly
docker compose -f docker-compose.prod.yml exec postgres psql -U obsrv -d obsrv -c "VACUUM ANALYZE;"

# Check partition sizes
docker compose -f docker-compose.prod.yml exec postgres psql -U obsrv -d obsrv -c "
  SELECT
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
  FROM pg_tables
  WHERE tablename LIKE 'product_history_%'
  ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
"
```

---

## Troubleshooting

### API Not Responding

```bash
# Check if container is running
docker compose -f docker-compose.prod.yml ps api

# Check logs for errors
docker compose -f docker-compose.prod.yml logs api --tail=100

# Restart API
docker compose -f docker-compose.prod.yml restart api
```

### Crawls Not Executing

```bash
# Check Celery worker status
docker compose -f docker-compose.prod.yml ps celery-worker

# Inspect Celery logs
docker compose -f docker-compose.prod.yml logs celery-worker --tail=100

# Check Celery Beat scheduler
docker compose -f docker-compose.prod.yml logs celery-beat --tail=50

# Verify scheduled tasks in Flower
# Open https://api.obsrv.example.com:5555/tasks
```

### Database Connection Issues

```bash
# Test database connectivity
docker compose -f docker-compose.prod.yml exec postgres pg_isready -U obsrv

# Check database logs
docker compose -f docker-compose.prod.yml logs postgres --tail=100

# Verify connection from API container
docker compose -f docker-compose.prod.yml exec api psql $DATABASE_URL -c "SELECT 1;"
```

### High Memory Usage

```bash
# Check memory usage
docker stats --no-stream

# Reduce Celery worker concurrency
# Edit docker-compose.prod.yml:
# celery-worker:
#   command: celery -A src.tasks worker --concurrency=2 --loglevel=info

# Restart workers
docker compose -f docker-compose.prod.yml restart celery-worker
```

### Webhook Delivery Failures

```bash
# Check webhook logs
docker compose -f docker-compose.prod.yml exec postgres psql -U obsrv -d obsrv -c "
  SELECT
    target_url,
    status,
    http_status_code,
    error_message,
    delivery_timestamp
  FROM webhook_delivery_logs
  WHERE status != 'success'
  ORDER BY delivery_timestamp DESC
  LIMIT 20;
"

# Retry failed webhooks
curl -X POST https://api.obsrv.example.com/v1/webhooks/<webhook-id>/retry \
  -H "X-API-Key: <your-api-key>"
```

---

## Security Best Practices

1. **API Keys**:
   - Generate unique API keys per client/integration
   - Rotate keys every 90 days
   - Never log API keys
   - Store keys in secure vault (e.g., HashiCorp Vault, AWS Secrets Manager)

2. **Webhook Secrets**:
   - Rotate webhook secrets regularly (90 days)
   - Always verify HMAC signatures on webhook receiver side
   - Use 1-hour grace period for zero-downtime rotation

3. **Database**:
   - Use strong passwords (32+ characters)
   - Enable SSL connections for remote access
   - Regular backups with encryption
   - Restrict network access (firewall rules)

4. **Network**:
   - Use HTTPS only (TLS 1.2+)
   - Configure firewall to allow only necessary ports
   - Rate limit API endpoints (use nginx rate limiting)
   - Block suspicious IPs using fail2ban

5. **Updates**:
   - Keep Docker and Docker Compose updated
   - Regularly update base images
   - Monitor security advisories for dependencies
   - Apply OS security patches monthly

---

## Performance Tuning

### API Server

```yaml
# docker-compose.prod.yml
services:
  api:
    environment:
      API_WORKERS: 4  # Increase for more concurrent requests (2 × CPU cores)
      UVICORN_TIMEOUT_KEEP_ALIVE: 65
```

### Celery Workers

```yaml
services:
  celery-worker:
    command: celery -A src.tasks worker --concurrency=4 --prefetch-multiplier=1 --loglevel=info
    # concurrency: Number of concurrent tasks (recommended: 1-2 per CPU core)
    # prefetch-multiplier: 1 for long-running tasks (crawls)
```

### PostgreSQL

```bash
# Edit postgresql.conf (via Docker volume mount)
shared_buffers = 2GB                # 25% of system RAM
effective_cache_size = 6GB          # 75% of system RAM
maintenance_work_mem = 512MB
checkpoint_completion_target = 0.9
wal_buffers = 16MB
default_statistics_target = 100
random_page_cost = 1.1              # For SSD
effective_io_concurrency = 200
work_mem = 10MB
max_connections = 100
```

### Redis

```bash
# Redis configuration via docker-compose
services:
  redis:
    command: redis-server --maxmemory 2gb --maxmemory-policy allkeys-lru
```

---

## Next Steps

1. **Setup Monitoring**: Integrate with Sentry for error tracking
2. **Configure Alerts**: Setup alerts for high failure rates, disk usage
3. **Backup Automation**: Implement automated daily backups
4. **Load Testing**: Run load tests to validate performance targets
5. **Documentation**: Share API documentation with clients (OpenAPI spec)

---

## Support

- **Documentation**: https://docs.obsrv.example.com
- **API Reference**: https://api.obsrv.example.com/docs (OpenAPI/Swagger UI)
- **Support Email**: support@obsrv.example.com
- **Status Page**: https://status.obsrv.example.com

---

**Status**: ✅ Quickstart guide complete - Ready for deployment
